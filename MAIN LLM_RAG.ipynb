{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYVvzGXfWAFI",
        "outputId": "e6111d25-df64-4edc-ca6a-cd5688064cfe"
      },
      "outputs": [],
      "source": [
        "!pip install -U -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5f00dEPWAFO",
        "outputId": "9c383b43-124f-4730-812b-8ece9537412f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")\n",
        "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"Enter your HF API key: \")\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter your Langchain API key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsdBuDkvWAFP",
        "outputId": "44127d7b-0c9b-44e7-926b-ecc8a364a2ee"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Replace this line\n",
        "llm = ChatGroq(\n",
        "    model=\"mixtral-8x7b-32768\",  # Updated model\n",
        "    temperature=0,\n",
        "    max_tokens=4000,\n",
        "    timeout=None\n",
        ")\n",
        "llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3V_kfpiWAFQ",
        "outputId": "d7fb75ff-70a5-467f-a612-f535556b6296"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import json\n",
        "import uuid\n",
        "import cassio  # Using cassio for AstraDB connection\n",
        "import pandas as pd\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# AstraDB connection details\n",
        "\n",
        "ASTRA_DB_APPLICATION_TOKEN =  getpass.getpass(\"Enter your Astra DB Token: \")\n",
        "ASTRA_DB_ID =  getpass.getpass(\"Enter your Astra DB ID: \")\n",
        "\n",
        "\n",
        "# Initialize AstraDB connection with Cassio\n",
        "cassio.init(\n",
        "    token=ASTRA_DB_APPLICATION_TOKEN,\n",
        "    database_id=ASTRA_DB_ID\n",
        ")\n",
        "\n",
        "# Embeddings setup\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "# ChatGroq LLM setup\n",
        "# Replace this line\n",
        "llm = ChatGroq(\n",
        "    model=\"mixtral-8x7b-32768\",  # Updated model\n",
        "    temperature=0,\n",
        "    max_tokens=4000,\n",
        "    timeout=None\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1Y7VzHaWAFT",
        "outputId": "20b1fb98-37bf-43d1-d784-4b81521d8628"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores.cassandra import Cassandra\n",
        "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.schema import Document\n",
        "import cassio\n",
        "\n",
        "# Create Cassandra vector store\n",
        "astra_vector_store = Cassandra(\n",
        "    embedding=embeddings,\n",
        "    session=None,\n",
        "    table_name=\"Projekt\",\n",
        "    keyspace=None\n",
        ")\n",
        "\n",
        "def process_csv(file_path, context_column, response_column):\n",
        "    df = pd.read_csv(file_path)\n",
        "    docs = []\n",
        "    for _, row in df.iterrows():\n",
        "        content = f\"Context: {row[context_column]}\\nResponse: {row[response_column]}\"\n",
        "        metadata = {\"source\": file_path, \"type\": \"csv\"}\n",
        "        docs.append(Document(page_content=content, metadata=metadata))\n",
        "    return docs\n",
        "\n",
        "def process_json(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    docs = []\n",
        "    for item in data:\n",
        "        content = f\"Question: {item['question']}\\nAnswer: {item['answer']}\\nTopic: {item['topic']}\"\n",
        "        metadata = {\"source\": file_path, \"type\": \"json\", \"topic\": item['topic']}\n",
        "        docs.append(Document(page_content=content, metadata=metadata))\n",
        "    return docs\n",
        "\n",
        "# Process and add documents\n",
        "docs = []\n",
        "docs.extend(process_csv('/content/amod.csv', 'Context', 'Response'))\n",
        "docs.extend(process_json('/content/cleaned_chats.json'))\n",
        "\n",
        "# Add documents to the vector store\n",
        "astra_vector_store.add_documents(docs)\n",
        "print(f\"Inserted {len(docs)} documents.\")\n",
        "\n",
        "# Create a vector store index wrapper\n",
        "astra_vector_index = VectorStoreIndexWrapper(vectorstore=astra_vector_store)\n",
        "\n",
        "print(\"Vector index created successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "CS8u8c4fb5Rc",
        "outputId": "305c9e2a-446d-4d2e-f9d6-b72e3fc26208"
      },
      "outputs": [],
      "source": [
        "\n",
        "import datetime\n",
        "import warnings\n",
        "from IPython.display import display, HTML\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "import logging\n",
        "\n",
        "\n",
        "# Filter out specific warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "logging.getLogger('cassandra.protocol').setLevel(logging.ERROR)\n",
        "\n",
        "\n",
        "# Memory setup\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "# Create the Conversational Retrieval Chain (RAG setup)\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=astra_vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "def format_message(role, content):\n",
        "    \"\"\"Formats the message with HTML for better display in Colab.\"\"\"\n",
        "    color = \"#4a76a8\" if role == \"Bot\" else \"#6c757d\"\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    return f\"\"\"\n",
        "\n",
        "        {role} - {timestamp}\n",
        "        {content}\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "def chat_with_rag():\n",
        "    print(\"Hi, how are you feeling today?, You can type 'exit' or 'thankyou' to end the conversation.\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \")\n",
        "\n",
        "        if user_input.lower() == 'exit' or 'thankyou':\n",
        "            display(HTML(format_message(\"Bot\", \"Goodbye! Have a great day!\")))\n",
        "            break\n",
        "\n",
        "        if not user_input.strip():\n",
        "            display(HTML(format_message(\"Bot\", \"Please enter a valid question or command.\")))\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            result = qa_chain.invoke({\"question\": user_input})\n",
        "            # display(HTML(format_message(\"You\", user_input)))\n",
        "            display(HTML(format_message(\"Bot\", result['answer'])))\n",
        "        except Exception as e:\n",
        "            error_message = f\"Oops! Something went wrong. Please try again.\\nError: {str(e)}\"\n",
        "            display(HTML(format_message(\"Bot\", error_message)))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    chat_with_rag()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "ZTfMyDHKEiY5",
        "outputId": "ccfcea92-d9d8-4feb-9c58-fdc794355350"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import warnings\n",
        "from IPython.display import display, HTML\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "import logging\n",
        "\n",
        "\n",
        "def format_message(role, content):\n",
        "    \"\"\"Formats the message with HTML for better display in Colab.\"\"\"\n",
        "    color = \"#4a76a8\" if role == \"Bot\" else \"#6c757d\"\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    return f\"\"\"\n",
        "    <div style=\"margin-bottom: 10px; padding: 10px; border-radius: 5px; background-color: {color}; color: white;\">\n",
        "        <strong>{role}</strong> - {timestamp}<br>\n",
        "        {content}\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "def chat_with_rag():\n",
        "    # Memory setup (moved inside the function to ensure it's defined)\n",
        "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "    # Create the Conversational Retrieval Chain (RAG setup) (moved inside the function)\n",
        "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=astra_vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
        "        memory=memory\n",
        "    )\n",
        "\n",
        "    print(\"Hi, how are you feeling today?, You can type 'exit' or 'thankyou' to end the conversation.\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \").strip()\n",
        "\n",
        "        if user_input.lower() in ['exit', 'thankyou']:\n",
        "            display(HTML(format_message(\"Bot\", \"Goodbye! Have a great day!\")))\n",
        "            break\n",
        "\n",
        "        if not user_input:\n",
        "            display(HTML(format_message(\"Bot\", \"Please enter a valid question or command.\")))\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            result = qa_chain.invoke({\"question\": user_input})\n",
        "            display(HTML(format_message(\"Bot\", result['answer'])))\n",
        "        except Exception as e:\n",
        "            error_message = f\"Oops! Something went wrong. Please try again.\\nError: {str(e)}\"\n",
        "            display(HTML(format_message(\"Bot\", error_message)))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    chat_with_rag()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaU2xpwSxT89",
        "outputId": "a97ade39-6fba-40b2-bd48-10949026dc6f"
      },
      "outputs": [],
      "source": [
        "# prompt: installl gradio\n",
        "\n",
        "!pip install gradio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "fn-rFN1gNY1o",
        "outputId": "f8ad455b-f023-42e1-b571-f82dcfc303cd"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import warnings\n",
        "import logging\n",
        "import gradio as gr\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "logging.getLogger('cassandra.protocol').setLevel(logging.ERROR)\n",
        "\n",
        "# Memory setup\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "# Create the Conversational Retrieval Chain (RAG setup)\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=astra_vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "def chatbot_response(user_input):\n",
        "    \"\"\"Handles chatbot response and returns formatted output.\"\"\"\n",
        "    if user_input.lower() in ['exit', 'thankyou']:\n",
        "        return \"Goodbye! Have a great day! ðŸ˜Š\"\n",
        "\n",
        "    if not user_input.strip():\n",
        "        return \"Please enter a valid question or command.\"\n",
        "\n",
        "    try:\n",
        "        result = qa_chain.invoke({\"question\": user_input})\n",
        "        return result['answer']\n",
        "    except Exception as e:\n",
        "        return f\"Oops! Something went wrong. Please try again.\\nError: {str(e)}\"\n",
        "\n",
        "# Gradio UI\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## ðŸ¤– Mental Health Chatbot\")\n",
        "    chatbot = gr.Chatbot()\n",
        "    user_input = gr.Textbox(label=\"Type your message here...\")\n",
        "\n",
        "    def update_chat(message, history):\n",
        "        response = chatbot_response(message)\n",
        "        history.append((message, response))\n",
        "        return \"\", history\n",
        "\n",
        "    user_input.submit(update_chat, [user_input, chatbot], [user_input, chatbot])\n",
        "\n",
        "# Launch the interactive UI\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
